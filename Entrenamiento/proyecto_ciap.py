# -*- coding: utf-8 -*-
"""Proyecto CIAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1utmFrJNuDBGKaZIZBfNJTXfpIqqL2XM_
"""

import pandas as pd

df = pd.read_csv('creditcard.csv')
df.head()

df.shape

df.info()

clase = df['Class'].value_counts()
clase

tiempo = df.groupby('Time')['Time'].count()
tiempo

df.describe()

"""# **Exploración de datos**
Análisis entre las diferentes versiones y amount para determinar su correlación entre las clases, es decir, fraude o no fraude.
"""

# CELDA MODIFICADA: Visualización del Desbalance de Clases (Escala Lineal)

# 1. Conteo de transacciones por clase
print("--- Conteo de Transacciones por Clase (0: Legítima, 1: Fraude) ---")
counts = df['Class'].value_counts()
print(counts)

# 2. Porcentaje de Fraude (Cuantificando el desbalance)
total_transacciones = counts.sum()
fraude_porcentaje = counts[1] / total_transacciones * 100
print(f"\nPorcentaje de Fraude: {fraude_porcentaje:.4f}%")


# 3. Gráfico de barras para visualizar la desproporción
plt.figure(figsize=(8, 6))
# Se usa la escala LINEAL (sin plt.yscale)
sns.barplot(x=counts.index, y=counts.values, palette=['skyblue', 'red'])

# Títulos y etiquetas
plt.title('Distribución de Clases: Desbalance Extremo (Escala Lineal)', fontsize=16)
plt.xticks(ticks=[0, 1], labels=['Legítima', 'Fraude'])
plt.ylabel('Número de Transacciones', fontsize=12)
plt.xlabel('Clase', fontsize=12)

# Añadir etiquetas de valor para asegurar una buena interpretación
for i, count in enumerate(counts.values):
    # Para la barra de Fraude, se mueve la etiqueta muy arriba para que sea visible
    y_pos = count + 20000 if i == 1 else count
    plt.text(i, y_pos, str(count), ha='center', va='bottom', fontsize=10)

plt.savefig('distribucion_lineal.png')
# plt.show()

# CELDA 4: Gráfico 2 - Distribución del Monto (Amount) por Clase

plt.figure(figsize=(10, 6))
# Usamos un boxplot para comparar la mediana y la dispersión
sns.boxplot(x='Class', y='Amount', data=df, palette=['skyblue', 'red'])

plt.title('Gráfico 2: Monto de Transacción por Clase')
plt.xticks(ticks=[0, 1], labels=['Legítima', 'Fraude'])
plt.ylabel('Monto de Transacción')
plt.xlabel('Clase')

# Limitar el eje Y es útil, ya que hay valores extremos (outliers)
plt.ylim(0, 500)
plt.show()

"""# **Pairplot**
Análisis entre las diferentes versiones y amount para determinar su correlación entre las clases, es decir, fraude o no fraude.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Seleccionar 5 variables (excluyendo 'Time' y 'Class') para el pairplot
# Puedes cambiar los nombres de las columnas en la siguiente lista si deseas otras variables
selected_cols = ['V1', 'V2', 'V3', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V4', 'V5', 'V6', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V7', 'V8', 'V9', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V9', 'V10', 'V11', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V12', 'V13', 'V14', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V15', 'V16', 'V17', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V18', 'V19', 'V20', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V21', 'V22', 'V23', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V24', 'V25', 'V26', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

selected_cols = ['V27', 'V28', 'Amount', 'Class']
sns.pairplot(df[selected_cols], hue='Class', plot_kws={'alpha': 0.2})
plt.show()

"""# Entrenamiento"""

from sklearn.model_selection import train_test_split

# Drop rows where 'Class' is NaN
df_cleaned = df.dropna(subset=['Class'])

X = df_cleaned.drop(columns=['Class', 'Time'])
y = df_cleaned['Class']

# 3) División train/test (estratificada por la clase debido al desbalance)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# 4) Inspección rápida de 'Amount'
print("Inspeccioón de amount en train:")
print(X_train['Amount'].describe())

print("Conteo de clases en y_train:")
print(y_train.value_counts())
print("\nConteo de clases en y_test:")
print(y_test.value_counts())

"""## Optimización con Optuna"""

!pip install optuna

import optuna
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score

def objective(trial):
    # Hiperparámetros a optimizar para XGBoost
    n_estimators = trial.suggest_int('n_estimators', 100, 1000)
    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)
    max_depth = trial.suggest_int('max_depth', 3, 10)
    subsample = trial.suggest_float('subsample', 0.6, 1.0)
    colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0)
    gamma = trial.suggest_float('gamma', 0.0, 5.0)
    reg_lambda = trial.suggest_float('reg_lambda', 0.0, 10.0)
    reg_alpha = trial.suggest_float('reg_alpha', 0.0, 10.0)

    # Crear el modelo XGBoost con los hiperparámetros sugeridos
    xgb_model_v1 = XGBClassifier(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        subsample=subsample,
        colsample_bytree=colsample_bytree,
        gamma=gamma,
        reg_lambda=reg_lambda,
        reg_alpha=reg_alpha,
        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
        objective='binary:logistic',
        eval_metric='auc',
        n_jobs=-1,
        random_state=42
    )

    score = cross_val_score(xgb_model_v1, X_train, y_train, cv=3, scoring='roc_auc', n_jobs=-1).mean()
    return score

study = optuna.create_study(direction='maximize')  # Queremos maximizar el AUC
study.optimize(objective, n_trials=10, n_jobs=-1)

print("Mejor ROC-AUC:", study.best_value)
print("Mejores hiperparámetros:")
print(study.best_params)

from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, recall_score

# Recuperar los mejores hiperparámetros de Optuna
best_params = study.best_params

# Crear el modelo XGBoost con los mejores hiperparámetros
best_model_optuna_new = XGBClassifier(
    n_estimators=best_params['n_estimators'],
    learning_rate=best_params['learning_rate'],
    max_depth=best_params['max_depth'],
    subsample=best_params['subsample'],
    colsample_bytree=best_params['colsample_bytree'],
    gamma=best_params['gamma'],
    reg_lambda=best_params['reg_lambda'],
    reg_alpha=best_params['reg_alpha'],
    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),
    objective='binary:logistic',
    eval_metric='auc',
    n_jobs=-1,
    random_state=42
)

# Entrenar el modelo con los datos de entrenamiento balanceados
print("Entrenando el modelo con los hiperparámetros óptimos...")
best_model_optuna_new.fit(X_train, y_train)
print("Modelo entrenado exitosamente.")

# Realizar predicciones en el conjunto de prueba
y_pred_optuna = best_model_optuna_new.predict(X_test)
y_pred_prob_optuna = best_model_optuna_new.predict_proba(X_test)[:,1]

# Evaluar el rendimiento del modelo optimizado
print("\n--- Evaluación del Modelo XGBoost con Hiperparámetros Optimizados por Optuna ---")
print(classification_report(y_test, y_pred_optuna))
print(f"ROC-AUC (Modelo Optimizado): {roc_auc_score(y_test, y_pred_prob_optuna):.4f}")
print("Confusion Matrix (Modelo Optimizado):\n", confusion_matrix(y_test, y_pred_optuna))

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, y_pred_optuna, labels=[0, 1])
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Fraude', 'Fraude'])

fig, ax = plt.subplots(figsize=(8, 6))
disp.plot(cmap=plt.cm.Blues, ax=ax)
ax.set_title('Matriz de Confusión del Modelo Optimizado')
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay

# Visualizaciones de Curvas ROC y PR
print("\n--- Visualizaciones del Modelo Optimizado ---")
# Curva ROC
RocCurveDisplay.from_estimator(best_model_optuna_new, X_test, y_test)
plt.title('Curva ROC para el Modelo Optimizado')
plt.show()

print("\n")

# Curva Precision-Recall
PrecisionRecallDisplay.from_estimator(best_model_optuna_new, X_test, y_test)
plt.title('Curva Precision-Recall para el Modelo Optimizado')
plt.show()

"""# Calibración"""

from sklearn.calibration import CalibrationDisplay
import matplotlib.pyplot as plt

# Crear la curva de calibración
plt.figure(figsize=(10, 6))
CalibrationDisplay.from_estimator(best_model_optuna_new, X_test, y_test, n_bins=800, strategy='quantile')
plt.title('Curva de Calibración de Probabilidad para el Modelo Optimizado')
plt.xlabel('Probabilidad Media')
plt.ylabel('Fracción de Positivos Observados (y_true = 1)')
plt.show()

from sklearn.calibration import CalibratedClassifierCV

# Calibración con validación cruzada interna
calibrated_model = CalibratedClassifierCV(best_model_optuna_new, method='isotonic', cv=2)
calibrated_model.fit(X_train, y_train)

from sklearn.calibration import calibration_curve
import matplotlib.pyplot as plt

# Probabilidades antes y después
y_proba_uncalibrated = best_model_optuna_new.predict_proba(X_test)[:, 1]
y_proba_calibrated = calibrated_model.predict_proba(X_test)[:, 1]

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# Realizar predicciones con el modelo calibrado para la matriz de confusión
y_pred_calibrated = calibrated_model.predict(X_test)

# Calcular y visualizar la Matriz de Confusión para el modelo calibrado
print("--- Matriz de Confusión del Modelo Calibrado ---")
cm_calibrated = confusion_matrix(y_test, y_pred_calibrated, labels=[0, 1])
disp_calibrated = ConfusionMatrixDisplay(confusion_matrix=cm_calibrated, display_labels=['No Fraude', 'Fraude'])

fig, ax = plt.subplots(figsize=(8, 6))
disp_calibrated.plot(cmap=plt.cm.Blues, ax=ax)
ax.set_title('Matriz de Confusión del Modelo Calibrado')
plt.show()

from sklearn.metrics import classification_report, roc_auc_score

print("\n--- Reporte de Clasificación del Modelo Calibrado ---")
print(classification_report(y_test, y_pred_calibrated))
print(f"ROC-AUC (Modelo Calibrado): {roc_auc_score(y_test, y_proba_calibrated):.4f}")

# Curvas de calibración
prob_true_uncal, prob_pred_uncal = calibration_curve(y_test, y_proba_uncalibrated, n_bins=800, strategy = 'quantile')
prob_true_cal, prob_pred_cal = calibration_curve(y_test, y_proba_calibrated, n_bins=800, strategy = 'quantile')

# Plot
plt.figure(figsize=(6, 4))
plt.plot(prob_pred_uncal, prob_true_uncal, "s-", label="Antes de calibrar")
plt.plot(prob_pred_cal, prob_true_cal, "o-", label="Después de calibrar")
plt.plot([0, 1], [0, 1], "k--", label="Perfectamente calibrado")
plt.xlabel("Probabilidad media predicha (P)")
plt.ylabel("Fracción de positivos observados (Y_true=1)")
plt.title("Curva de Calibración de Probabilidad")
plt.legend()
plt.grid(True)
plt.show()

"""# Guardar modelo"""

import joblib

from google.colab import drive
drive.mount('/content/drive')

# Define the filename for the saved model in Google Drive
#model_filename = 'fraud_detection_model_v2.joblib'
model_filename = '/content/drive/MyDrive/fraud_detection_model_optuna_calibrated_v3.joblib' # Changed filename for clarity

# Save the model to a file
joblib.dump(calibrated_model, model_filename)

print(f"Model saved successfully to {model_filename}")

"""## **Explicabilidad**
Para la explicabilidad (XAI), se va integrar SHAP (SHapley Additive exPlanations) , permitirá justificar por qué modelo XGBoost clasifica una transacción como fraude, cumpliendo con el estándar de la industria.
"""

!pip install shap

import shap
# Usa un background pequeño (por ejemplo 50 muestras reales)
background = X_test.sample(50, random_state=42)
model_predict = lambda x: calibrated_model.predict_proba(x)[:,1]  # prob de la clase positiva

explainer = shap.KernelExplainer(model_predict, background.values)
# calcula para un pequeño subset (kernel explainer es costoso)
X_sample = X_test.sample(20, random_state=1)
shap_values = explainer.shap_values(X_sample.values, nsamples=200)
# plot (usa shap.force_plot o summary_plot con arreglos)
shap.summary_plot(shap_values, X_sample)

# --- 1. SHAP Bar Plot (Importancia global de las características) ---
print("Generando SHAP Bar Plot...")
plt.figure(figsize=(10, 7))
# shap_values[1] for the positive class (fraude)
shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False)
plt.title('Importancia Global de las Características (SHAP Bar Plot)')
plt.tight_layout()
plt.show()

if isinstance(shap_values, list):
    mean_abs_shap_values = np.abs(shap_values[1]).mean(axis=0)
else:
    mean_abs_shap_values = np.abs(shap_values).mean(axis=0)

# Get the feature names from X_sample
feature_names = X_sample.columns.tolist()
# Create a Series for easier sorting
feature_importance = pd.Series(mean_abs_shap_values, index=feature_names)
# Sort features by importance
sorted_features = feature_importance.sort_values(ascending=False)

# Seleccionamos las dos características más importantes para el dependence plot
feature_to_plot = sorted_features.index[0] # La más importante
interaction_feature = sorted_features.index[1] # La segunda más importante

print(f"Generando SHAP Dependence Plot para {feature_to_plot} (interacción con {interaction_feature})...")
plt.figure(figsize=(10, 7))
# shap_values[1] for the positive class (fraude) if it's a list
# Otherwise, use shap_values directly if it's already for the positive class
shap_values_for_plot = shap_values[1] if isinstance(shap_values, list) else shap_values

shap.dependence_plot(
    feature_to_plot, shap_values_for_plot, X_sample,
    interaction_index=interaction_feature,
    title=f'SHAP Dependence Plot: {feature_to_plot} vs {interaction_feature}', show=False
)
plt.tight_layout()
plt.show()